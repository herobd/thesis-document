\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Examples of word spotting for `pay' and `payment'.\relax }}{2}{figure.caption.1}
\contentsline {figure}{\numberline {1.2}{\ignorespaces Examples of subword spotting for the character trigram `pay' (left, red) and bigram `pa' (right, yellow).\relax }}{2}{figure.caption.2}
\contentsline {figure}{\numberline {1.3}{\ignorespaces Cropped examples of the characters ``e'' and ``i'' (excluding dots), on the left and right respectively, from a single author. Without context the characters are practically indistinguishable.\relax }}{4}{figure.caption.3}
\contentsline {figure}{\numberline {1.4}{\ignorespaces An example of a word having ``pa'' and ``men'' spotted in it. A regular expression representing this, \texttt {pa..?men..?}, yields only a few matches from our lexicon, including the correct one: ``pavement'', ``pavements'', ``\textbf {payment}'', and ``payments''.\relax }}{5}{figure.caption.4}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Example of how vertical slice window features are extracted. Typically most features are extracted on a binarized image. Deskewing plays an important role as the vertical slices are very sensitive to skew. Many features extracted are pixel counts; in this example we show count features dependent on baselines (blue).\relax }}{8}{figure.caption.5}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Example of a three level PHOC vector for the word ``face''. The final vector is all levels appended together. Note that partial values are given when characters are split over bins.\relax }}{9}{figure.caption.6}
\contentsline {figure}{\numberline {2.3}{\ignorespaces A screenshot of a demo of Toselli et al's multimodal CAT system. The red line is drawn by the user to indicate the need to insert a word into the automatically obtained transcription.\relax }}{11}{figure.caption.7}
\contentsline {figure}{\numberline {2.4}{\ignorespaces Clawson's CAT system for tabular documents.\relax }}{13}{figure.caption.8}
\contentsline {figure}{\numberline {2.5}{\ignorespaces A screen-shot of a the CAT system of Zagoris et al.\nobreakspace {}\cite {Zagoris2015}. The green text indicates hand labeling, the blue text indicates automatic labeling. You can see to the right of the current word (``must'') the current ranked list of spottings (blue boxes).\relax }}{13}{figure.caption.9}
\contentsline {figure}{\numberline {2.6}{\ignorespaces A screen-shot of a character session for ``?'' from Neudecker and Tzadok's CAT system, taken directly from their report \cite {Neudecker2010}. Notice how easy it is for a user to simply click on the erroneous classifications.\relax }}{15}{figure.caption.10}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Examples of lines from the IAM dataset.\relax }}{18}{figure.caption.11}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Excerpts from the Bentham dataset.\relax }}{19}{figure.caption.12}
\contentsline {figure}{\numberline {3.3}{\ignorespaces Excerpts from the Census Names dataset.\relax }}{21}{figure.caption.13}
\contentsline {figure}{\numberline {3.4}{\ignorespaces Example of hand annotated character segmentation.\relax }}{22}{figure.caption.14}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Network architecture for embedding images as PHOC vectors. The numbers beneath each layer represent the number of channels. As the network uses spatial pyramid pooling before the fully connected layers, it can accept images of any size.\relax }}{24}{figure.caption.15}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Example of data augmentation used in training.\relax }}{25}{figure.caption.16}
\contentsline {figure}{\numberline {4.3}{\ignorespaces MAP for QbS subword spotting results on the Bentham and Census Names datasets, where we only use n-grams in our query set which have an occurrence count above a threshold (in the testing sets). MAP increases, demonstrating less frequent n-grams are more difficult. Only trigrams are shown for the Bentham dataset due to the high frequency of all unigrams and bigrams.\relax }}{30}{figure.caption.19}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Unigram MAP when spotting results of consecutive image queries are combine (blue). The red line represents QbS performance and the yellow the average performance of all 50 QbE queries.\relax }}{31}{figure.caption.21}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Bigram MAP when spotting results of consecutive image queries are combine (blue). The red line represents QbS performance and the yellow the average performance of all 50 QbE queries.\relax }}{31}{figure.caption.22}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Trigram MAP when spotting results of consecutive image queries are combine (blue). The red line represents QbS performance and the yellow the average performance of all 50 QbE queries.\relax }}{32}{figure.caption.23}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces A figure with two subfigures\relax }}{35}{figure.caption.25}
\contentsline {figure}{\numberline {5.2}{\ignorespaces An overview of the CAT system which uses approved subword spottings. The arrows represent the flow of data.\relax }}{36}{figure.caption.26}
\addvspace {10\p@ }
